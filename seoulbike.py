# -*- coding: utf-8 -*-
"""SeoulBike.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J-W2C6-cCwfO3GWbsoxKj_B_rMPVCCNI
"""

from google.colab import drive
import sys

# Mount Google Drive
drive.mount('/content/drive')

# Get the absolute path of the current folder
abspath_curr = '/content/drive/My Drive/DATS 6313 - Final Project'

import pandas as pd
import datetime
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from sklearn.preprocessing import StandardScaler,LabelEncoder
from statsmodels.tsa.stattools import adfuller as ADF
from statsmodels.tsa.stattools import kpss
from tabulate import tabulate 
from sklearn.model_selection import train_test_split
import seaborn as sns
from numpy import linalg as la
from sklearn import preprocessing
import statsmodels.api as sm
from scipy import signal
from statsmodels.graphics.tsaplots import plot_acf , plot_pacf
from scipy.stats import chi2

"""# Data Description

## Read the Data
"""

df=pd.read_csv(abspath_curr + '/data/SeoulBikeData.csv', header=0)

"""## Inspect the Data"""

df.head()

df.info()

"""### **Description of independent variable (Numerical variables)**"""

df.drop(columns=['Rented Bike Count']).describe()

"""### Description of independent variable (Categorical)"""

df['Date'].describe()

df['Seasons'].describe()

df['Holiday'].describe()

df['Functioning Day'].describe()

"""### **Description of Dependent variable**"""

df['Rented Bike Count'].describe()

"""## Data Pre-processing

### Create Datetime Column
"""

df_clean=df.copy()

df_clean['Date']=df_clean['Date'].astype(str)
df_clean['Hour']=df_clean['Hour']*100
df_clean.loc[:,'Hour(str)']=df_clean['Hour'].map('{:04d}'.format).astype(str)
df_clean['datetime']=df_clean['Date']+df_clean['Hour(str)']
df_clean['datetime']=pd.to_datetime(df_clean['datetime'],format='%d/%m/%Y%H%M')

"""### Set Datetime as index"""

data_clean=df_clean.set_index('datetime').drop(columns=['Date','Hour','Hour(str)'])

"""### Encoding Categorical Vairiables

**Converting 'Holiday' and 'Functioning Day' to binary values**
"""

data_tf=data_clean.copy()
data_tf['Holiday']=data_clean['Holiday'].replace({'Holiday':1,'No Holiday':0})
data_tf['Functioning Day']=data_clean['Functioning Day'].replace({'Yes':1,'No':0})

"""**Converting 'Seasons' to dummy variables**"""

data_tf=pd.get_dummies(data=data_tf,columns=['Seasons'])
data=data_tf

"""## Plot Rented Bike Count vs. Time"""

rent=data['Rented Bike Count']
plt.figure(figsize=(16,8))
plt.plot(rent)
plt.title('Rented Bike Count vs. datetime')
plt.xlabel('Time')
plt.ylabel('Number of Bike Rented')
plt.tight_layout()
plt.show()

"""## ACF and PACF for Rent """

def ACF_PACF_Plot(y,lags):
    acf = sm.tsa.stattools.acf(y, nlags=lags)
    pacf = sm.tsa.stattools.pacf(y, nlags=lags)
    fig = plt.figure()
    plt.subplot(211)
    plt.title('ACF/PACF of the raw data')
    plot_acf(y, ax=plt.gca(), lags=lags)
    plt.subplot(212)
    plot_pacf(y, ax=plt.gca(), lags=lags)
    fig.tight_layout(pad=3)
    plt.show()
ACF_PACF_Plot(rent,50)

"""## Correlation Matrix"""

plt.figure(figsize=(14,14))
corr_mat=data.corr()
sns.heatmap(corr_mat,annot=True)
plt.title('Heatmap of Correlation Matrix')

"""## Train Test Split"""

y=data['Rented Bike Count']
x=data.drop(columns=['Rented Bike Count'])
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=12,shuffle=False)

x_train.head()

x_test.head()

"""# Check for stationarity

## ACF and PACF Analysis
"""

ACF_PACF_Plot(y_train,70)

"""## ADF Test for Rent"""

def display_adf_result(y):
    adf_result=ADF(y)
    print("ADF Statistics:",round(adf_result[0],6))
    print("P-value:",round(adf_result[1],6))
    print("Critical Values:")
    for k,v in adf_result[4].items():
        print("  ",k,":",round(v,3))
display_adf_result(y_train)

"""## KPSS Test for Rent"""

def kpss_test(timeseries):
    print ('Results of KPSS Test:')
    kpsstest = kpss(timeseries, regression='ct', nlags="auto")
    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','LagsUsed'])
    for key,value in kpsstest[3].items():
        kpss_output['Critical Value (%s)'%key] = value
    print (kpss_output)
kpss_test(y_train)

"""## Rolling mean and Rolling variance for Rent"""

def plot_rolling_mean_var(y):
    mean_r=[]
    var_r=[]
    for i in range(1,len(y)+1):
        mean_r.append(np.mean(y[:i]))
        var_r.append(np.var(y[:i]))
    plt.subplot(2,1,1)
    plt.plot(mean_r)
    plt.xlabel("Samples")
    plt.ylabel("Magnitude")
    plt.title("Rolling Mean")
    plt.subplot(2,1,2)
    plt.plot(var_r)
    plt.xlabel("Samples")
    plt.ylabel("Magnitude")
    plt.title("Rolling Variance")
    plt.tight_layout()
    plt.show()

plot_rolling_mean_var(y_train)

"""**KPSS indicates non-stationarity and ADF indicates stationarity - The series is difference stationary. Differencing is to be used to make series stationary. The differenced series is checked for stationarity.**

## Make the Data Stationary

### Take the first order differencing of the data
"""

y_diff1=y_train.diff(periods=1)[1:]

"""### Check for stationarity of differenced data

**ACF and PACF Analysis on differenced data**
"""

ACF_PACF_Plot(y_diff1,70)

"""**Perform ADF test on differenced data**"""

display_adf_result(y_diff1)

"""**Perform KPSS test on differenced data**"""

kpss_test(y_diff1)

"""**Plot Rolling mean and Rollong variance for differeced data**"""

plot_rolling_mean_var(y_diff1)

"""# Time Series Decomposition

## Decomposition using STL
"""

from statsmodels.tsa.seasonal import STL
STL=STL(y_train)
plt.figure(figsize=(32,32))
res=STL.fit()
fig=res.plot()
plt.tight_layout()
plt.show()

T = res.trend
S = res.seasonal
R = res.resid

"""## Calculate Strength of Trend and Seasonality"""

def cal_strength(T,S,R):
    F=np.maximum(0,1-np.var(np.array(R))/np.var(np.array(T+R)))
    Fs=np.maximum(0,1-np.var(np.array(R))/np.var(np.array(S+R)))

    print(f'The strength of trend for this data set is {100*F:.2f}%')
    print(f'The strength of seasonality for this data set is {100 * Fs:.2f}%')
cal_strength(T,S,R)

"""## Plot Seasonally Adjusted Data

### Additive Decomposition
"""

plt.figure(figsize=(32,16))
adj_y_add=y_train-S-T
plt.plot(y_train,label="Original Data")
plt.plot(adj_y_add,label="Seasonally Adjusted Data")
plt.title("Plot of Detrended and Seasonally Adjusted Data Under Additive Decomposition vs. Original Data",fontsize=40)
plt.legend(prop={'size':30})
plt.xlabel("Date")
plt.ylabel("Temp")
plt.tight_layout()
plt.show()

"""### Multiplicative Decomposition"""

adj_y_mul=y_train/S/T
plt.figure(figsize=(16,8))
plt.plot(y_train,label="Original Data")
plt.plot(adj_y_mul,label="Seasonally Adjusted Data")
plt.title("Plot of Detrended and Seasonally Adjusted Data Under Multiplicative Decomposition vs. Original Data")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Temp")
plt.tight_layout()
plt.show()

"""# Holt-Winters Method

## Using Default heurisitic method

### Fit the model
"""

import statsmodels.tsa.holtwinters as ets
holtt=ets.ExponentialSmoothing(y_train,trend="add", seasonal = "add",initialization_method='heuristic',damped=True).fit()
#holtt=ets.ExponentialSmoothing(y_diff1).fit()
holtf=holtt.forecast(steps = len(y_test))
holtf=pd.DataFrame(holtf).set_index(y_test.index)

fig, ax= plt.subplots()
ax.plot(y_train, label = "Train Data")
ax.plot(y_test, label = "Test Data")
ax.plot(holtf, label = "Holt Linear")
plt.title('Plot of Holt-Winters Forecasting')
plt.xlabel('Time')
plt.ylabel('Rented Bike Count')
plt.legend()
plt.show()

"""### Calculating Mean Squared Error for forecast result"""

holt_mse=np.mean((y_test-holtf[0])**2)
print('The Mean Squared Error for Holt-Winters Method with default parameters is:',holt_mse)

"""## Define moethod with infromation from y train

### Fit the model
"""

import statsmodels.tsa.holtwinters as ets
holtt1=ets.ExponentialSmoothing(y_train,trend="add", seasonal = "add",initialization_method='known',
                               initial_level=y_train[-1],initial_trend=T[-1],initial_seasonal=S[-1],damped=True).fit()
#holtt=ets.ExponentialSmoothing(y_diff1).fit()
holtf1=holtt1.forecast(steps = len(y_test))
holtf1=pd.DataFrame(holtf1).set_index(y_test.index)

fig, ax= plt.subplots()
ax.plot(y_train, label = "Train Data")
ax.plot(y_test, label = "Test Data")
ax.plot(holtf1, label = "Holt Linear")
plt.title('Plot of Holt-Winters Forecasting')
plt.xlabel('Time')
plt.ylabel('Rented Bike Count')
plt.legend()
plt.show()

"""### Calculating Mean Squared Error for forecast result"""

holt1_mse=np.mean((y_test-holtf1[0])**2)
print('The Mean Squared Error for Holt-Winters Method with default parameters is:',holt1_mse)

"""# Feature Selection

## Singular Value Decomposition
"""

x_mat=sm.add_constant(x_train).values
y_mat=np.array([y_train]).T
H=x_mat.T@x_mat
H_inv=la.inv(H)
u,s,vh=la.svd(H)
print(s)

"""**One low sigular values**

## Print the Condition Number
"""

print(f'The condition number for the data is {la.cond(x_train)}')

"""**Condition Number is Large, colinearality exist in feature space**

## Backward Stepwise Regression

### OLS Regression

### Sqare root Transformation
"""

plt.hist(y_train)
plt.title("Histogram of y train")
plt.xlabel("Rented Bike Count")
plt.ylabel("Frequency")

plt.hist(np.sqrt(y_train))
plt.title("Histogram of quare root of y train")
plt.xlabel("Rented Bike Count")
plt.ylabel("Frequency")

#x_ols=sm.add_constant(x_train)
x_ols=x_train
ols_model=sm.OLS(np.sqrt(y_train),x_ols)
ols_result=ols_model.fit()
print(ols_result.summary())

ols_model1=sm.OLS(np.sqrt(y_train),x_ols.drop(columns=['Visibility (10m)']))
ols_result1=ols_model1.fit()
print(ols_result1.summary())

ols_model2=sm.OLS(np.sqrt(y_train),x_ols.drop(columns=['Seasons_Winter','Visibility (10m)']))
ols_result2=ols_model2.fit()
print(ols_result2.summary())

ols_model3=sm.OLS(np.sqrt(y_train),x_ols.drop(columns=['Seasons_Winter','Visibility (10m)','Seasons_Summer']))
ols_result3=ols_model3.fit()
print(ols_result3.summary())

ols_model4=sm.OLS(np.sqrt(y_train),x_ols.drop(columns=['Seasons_Winter','Visibility (10m)',
                                                       'Seasons_Summer','Snowfall (cm)']))
ols_result4=ols_model4.fit()
print(ols_result4.summary())

ols_model5=sm.OLS(np.sqrt(y_train),x_ols.drop(columns=['Seasons_Winter','Visibility (10m)',
                                                       'Seasons_Summer','Snowfall (cm)','Temperature(C)']))
ols_result5=ols_model5.fit()
print(ols_result5.summary())

"""#### Not adding constant without squareroot transfromation"""

#Not adding constant
x_ols=x_train
ols_model=sm.OLS(y_train,x_ols)
ols_result=ols_model.fit()
print(ols_result.summary())

ols_model1=sm.OLS(y_train,x_ols.drop(columns=['Seasons_Spring']))
ols_result1=ols_model1.fit()
print(ols_result1.summary())

ols_model2=sm.OLS(y_train,x_ols.drop(columns=['Seasons_Spring','Visibility (10m)']))
ols_result2=ols_model2.fit()
print(ols_result2.summary())

ols_model3=sm.OLS(y_train,x_ols.drop(columns=['Seasons_Spring','Visibility (10m)','Dew point temperature(C)']))
ols_result3=ols_model3.fit()
print(ols_result3.summary())

"""#### Adding constant without transformation"""

x_ols=sm.add_constant(x_train)
#x_ols=x_train
ols_model=sm.OLS(y_train,x_ols)
ols_result=ols_model.fit()
print(ols_result.summary())

"""**With the regression summary, we can see that Visibility has the highest p-value, indicating that it is not as significant to predict 'Rented Bike Count' as other features, so we want to drop it to reduced feature space.**"""

ols_model1=sm.OLS(y_train,x_ols.drop(columns=['Visibility (10m)']))
ols_result1=ols_model1.fit()
print(ols_result1.summary())

"""**Drop Dew point temperature(C)**"""

ols_model2=sm.OLS(y_train,x_ols.drop(columns=['Visibility (10m)','Dew point temperature(C)']))
ols_result2=ols_model2.fit()
print(ols_result2.summary())

"""**Drop Seasons_Spring**"""

ols_model3=sm.OLS(y_train,x_ols.drop(columns=['Visibility (10m)','Dew point temperature(C)','Seasons_Spring']))
ols_result3=ols_model3.fit()
print(ols_result3.summary())

"""**The p-values for all the rest features are significantly small except for the constant, we will use this as the final model for OLS backward regression with constant added**

### VIF Regression
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
x_ols=x_train
ols_model=sm.OLS(np.sqrt(y_train),x_ols)
ols_result=ols_model.fit()
vif_result1=pd.DataFrame()
vif_result1['feature']=x_train.columns
vif_result1['VIF']=[variance_inflation_factor(x_train.values,i)
                    for i in  range(len(x_train.columns))]
print(ols_result.summary())
print(vif_result1)

"""**Drop Season Summer**"""

vif_result2=pd.DataFrame()
#x_vif=sm.add_constant(x_train)
x_vif=x_train
vif_result2['feature']=x_train.columns.drop('Seasons_Summer')
vif_result2['VIF']=[variance_inflation_factor(x_train.drop(columns=['Seasons_Summer']).values,i)
                    for i in  range(len(x_train.drop(columns=['Seasons_Summer']).columns))]

vif_model2=sm.OLS(np.sqrt(y_train),x_vif.drop(columns=['Seasons_Summer']))
vif_sum2=vif_model2.fit()
print(vif_sum2.summary())
print(vif_result2)

"""**Drop Functioning Day**"""

vif_result3=pd.DataFrame()
vif_result3['feature']=x_train.columns.drop(['Seasons_Summer','Functioning Day'])
vif_result3['VIF']=[variance_inflation_factor(x_train.drop(columns=['Seasons_Summer','Functioning Day']).values,i)
                    for i in range(len(x_train.drop(columns=['Seasons_Summer','Functioning Day']).columns))]

vif_model3=sm.OLS(np.sqrt(y_train),x_vif.drop(columns=['Seasons_Summer','Functioning Day']))
vif_sum3=vif_model3.fit()
print(vif_sum3.summary())
print(vif_result3)

"""**Drop Temerature**"""

vif_result4=pd.DataFrame()
vif_result4['feature']=x_train.columns.drop(['Seasons_Summer','Functioning Day','Dew point temperature(C)'])
vif_result4['VIF']=[variance_inflation_factor(x_train.drop(columns=['Seasons_Spring','Functioning Day','Dew point temperature(C)']).values,i)
                    for i in range(len(x_train.drop(columns=['Seasons_Spring','Functioning Day','Dew point temperature(C)']).columns))]

vif_model4=sm.OLS(np.sqrt(y_train),x_vif.drop(columns=['Seasons_Spring','Functioning Day','Dew point temperature(C)']))
vif_sum4=vif_model4.fit()
print(vif_sum4.summary())
print(vif_result4)

"""**Since the Adjusted R-square is decresing as we reduce more features, we will use the OLS with the highest Adjusted R-square value as the final model for VIF backward regression**

### Feature Selection Conclusion
"""

x_ols.drop(columns=['Seasons_Winter','Visibility (10m)','Seasons_Summer','Snowfall (cm)']).columns

x_ols.columns

"""**Comparing the results for backward regression feature selection based on the Adjusted R-square value, AIC and BIC, we choose the features 'Temperature(C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)', 'Holiday', 'Functioning Day', 'Seasons_Autumn','Seasons_Spring', 'Seasons_Summer', 'Seasons_Winter' as our features to fit linear regression on predicting Rents**"""

x_mat=x_ols.drop(columns=['Seasons_Winter','Visibility (10m)','Seasons_Summer','Snowfall (cm)']).values
y_mat=np.array([y_train]).T
H=x_mat.T@x_mat
H_inv=la.inv(H)
u,s,vh=la.svd(H)
print(s)
x_new=x_ols.drop(columns=['Seasons_Winter','Visibility (10m)','Seasons_Summer','Snowfall (cm)'])

"""**We sucessfully get rid of the sigular values that are relavantly small, meaning that we have reduced collinearity within our feature space**"""

print(f'The condition number for the data is {la.cond(x_new)}')

"""**The condition number for our feature matrix also decreases**

# Multiple Liner Regression

## Fit the mode

**Based on the features we selected, fit a Ordinary Lease Square Regression**
"""

ols_model=sm.OLS(np.sqrt(y_train),x_ols.drop(columns=['Seasons_Winter','Visibility (10m)',
                                                       'Seasons_Summer','Snowfall (cm)']))
ols_result=ols_model.fit()
print(ols_result.summary())

"""## Compare Forecase result with test data"""

ols_pred=ols_result.predict(x_test.drop(columns=['Seasons_Winter','Visibility (10m)',
                                                       'Seasons_Summer','Snowfall (cm)']))**2
ols_mse=np.mean((y_test-ols_pred)**2)
print("The mean squared error for multiple linear regression is :",ols_mse)

plt.plot(y_train,color="blue",label="Training Dataset")
plt.plot(y_test,color="orange",label="Testing Dataset")
plt.plot(ols_pred,color="green",label="Multiple Linear Regression Forecast")
plt.legend()
plt.title("Multiple Linear Regression Forecasting")
plt.xlabel("time - t")
plt.ylabel("Rented Bike Count")
plt.show()

"""## Hypothesis Tests

### t-test
"""

#ols_result.t_test(x_test.drop(columns=['Visibility (10m)','Dew point temperature(C)','Seasons_Spring'])).shape
r = np.eye(len(ols_result.params))
tt=ols_result.t_test(r)
print(tt.summary())

"""### F-test"""

ft=ols_result.f_test(np.identity(len(ols_result.params)))
print(ft.summary())

"""**Since the p-value for our F-test is significantly small, we conclude that the included features in our linear regression contributes to better predict our independent variable**

## Evaluatation Values
"""

print("The AIC of this model is:",ols_result.aic)
print("The BIC of this model is:",ols_result.bic)
print("The Adjusted R-square of this model is:",ols_result.rsquared_adj)
print("The R-square of this model is:",ols_result.rsquared)
print("The RMSE of this model is:",ols_mse**0.5)

"""## ACF Residual

**Plot the ACF of Residual with a lag of 50**
"""

ols_resid=ols_result.resid
n_lag=50
ry=list(sm.tsa.stattools.acf(ols_resid, nlags=n_lag))
ryy=ry[::-1][:-1]+ry
m=1.96/np.sqrt(n_lag)
lag=np.arange(-n_lag,n_lag+1,1)
plt.stem(lag,ryy)
plt.axhspan(-m,m,alpha=0.2)
plt.grid()
plt.xlabel("lags")
plt.ylabel("magnitude")
plt.title("Plot of residual error ACF")
plt.show()

"""## Q-value"""

ols_qvalue=sm.stats.acorr_ljungbox(ols_resid,lags=[n_lag],return_df=True)
print(ols_qvalue)

"""## Variane and Mean of Residual"""

#Print Residual Variance and Mean
ols_pred_mean=np.mean(ols_resid)
ols_pred_var=np.var(ols_resid)

print("The Mean of Multiple Linear regression residual is {}, \nand the variance of Multiple Linear regression residual is {}"
      .format(ols_pred_mean,ols_pred_var))

"""# Base Models

## Average Method
"""

def AverageForecast(y_train,y_test):
    y_train = y_train.tolist()
    y_test = y_test.tolist()
    train_pred = ['NA']
    for i in range(1,len(y_train)):
        train_pred.append(sum(y_train[:i])/i)
    train_error = np.array(y_train[2:])-np.array(train_pred[2:])
    test_pred = []
    for y in y_test:
        test_pred.append(np.mean(y_train))
    test_error = np.array(y_test[2:])-np.array(test_pred[2:])
    return train_pred,test_pred,train_error,test_error

avg_train_pred,avg_test_pred,avg_train_err,avg_test_err = AverageForecast(y_train,y_test)
plt.figure(figsize=(16,8))
plt.plot(y_train,color="blue",label="Training Dataset")
plt.plot(y_test,color="orange",label="Testing Dataset")
plt.plot(y_test.index,avg_test_pred,color="green",label="Average Method in h-step Prediction")
plt.legend()
plt.title("Average Method & Forecast")
plt.xlabel("time - t")
plt.ylabel("y value")
plt.show()
avg_mse=np.mean(avg_test_err**2)

#print Residual Error and Forecast Error
avg_pred_mse=np.mean(avg_train_err**2)
avg_forecast_mse=np.mean(avg_test_err**2)
print(tabulate([["Prediction",avg_pred_mse],["Forecast",avg_forecast_mse]],headers=["Average Method MSE"]))
#Print Residual Variance and Forecast variance
avg_pred_var=np.var(avg_train_err)
avg_forecast_var=np.var(avg_test_err)
print("The variance of average method prediction error is {}, \nand the variance of average method forecast error is {}"
      .format(avg_pred_var,avg_forecast_var))

#Calculate Q-value
avg_qvalue=sm.stats.acorr_ljungbox(avg_train_err,lags=n_lag,return_df=True)
print(avg_qvalue.iloc[-1])

"""## Naive Method"""

def NaiveForecast(y_train,y_test):
    y_train = y_train.tolist()
    y_test = y_test.tolist()
    train_pred = ['NA']
    for i in range(1,len(y_train)):
        train_pred.append(y_train[i-1])
    train_error = np.array(y_train[2:])-np.array(train_pred[2:])
    test_pred = []
    for y in y_test:
        test_pred.append(y_train[-1])
    test_error = np.array(y_test[2:])-np.array(test_pred[2:])
    return train_pred,test_pred,train_error,test_error

naive_train_pred,naive_test_pred,naive_train_err,naive_test_err = NaiveForecast(y_train,y_test)
plt.plot(y_train,color="blue",label="Training Dataset")
plt.plot(y_test,color="orange",label="Testing Dataset")
plt.plot(y_test.index,naive_test_pred,color="green",label="Naive Method in h-step Prediction")
plt.legend()
plt.title("Naive Method & Forecast")
plt.xlabel("time - t")
plt.ylabel("y value")
plt.show()

#Prediction and Forecast MSE
naive_pred_mse=np.mean(naive_train_err**2)
naive_forecast_mse=np.mean(naive_test_err**2)
print(tabulate([["Prediction",naive_pred_mse],["Forecast",naive_forecast_mse]],headers=["Naive Method MSE"]))

#Prediction and Forecast Variance
naive_pred_var=np.var(naive_train_err)
naive_forecast_var=np.var(naive_test_err)
print("The variance of naive method prediction error is {}, \nand the variance of naive method forecast error is {}"
      .format(naive_pred_var,naive_forecast_var))

#Q-value
naive_qvalue=sm.stats.acorr_ljungbox(naive_train_err,lags=n_lag,return_df=True)
print(naive_qvalue.iloc[-1])

"""## Drift Method"""

def DriftForecast(y_train,y_test):
    y_train = y_train.tolist()
    y_test = y_test.tolist()
    train_pred = ['NA','NA']
    for i in range(2,len(y_train)):
        train_pred.append(y_train[i-1]+(y_train[i-1]-y_train[0])/(i-1))
    train_error = np.array(y_train[2:])-np.array(train_pred[2:])
    test_pred = []
    for j in range(1,len(y_test)+1):
        test_pred.append(y_train[-1]+j*(y_train[-1]-y_train[0])/(len(y_train)-1))
    test_error = np.array(y_test[2:])-np.array(test_pred[2:])
    return train_pred,test_pred,train_error,test_error

drift_train_pred,drift_test_pred,drift_train_err,drift_test_err = DriftForecast(y_train,y_test)
plt.plot(y_train,color="blue",label="Training Dataset")
plt.plot(y_test,color="orange",label="Testing Dataset")
plt.plot(y_test.index,drift_test_pred,color="green",label="Naive Method in h-step Prediction")
plt.legend()
plt.title("Drift Method & Forecast")
plt.xlabel("time - t")
plt.ylabel("y value")
plt.show()

#Prediction and Forecast MSE
drift_pred_mse=np.mean(drift_train_err**2)
drift_forecast_mse=np.mean(drift_test_err**2)
print(tabulate([["Prediction",drift_pred_mse],["Forecast",drift_forecast_mse]],headers=["Drift Method MSE"]))

#Prediction and Forecast Variance
drift_pred_var=np.var(drift_train_err)
drift_forecast_var=np.var(drift_test_err)
print("The variance of drift method prediction error is {}, \nand the variance of drift method forecast error is {}"
      .format(drift_pred_var,drift_forecast_var))

#Q-value
drift_qvalue=sm.stats.acorr_ljungbox(drift_train_err,lags=n_lag,return_df=True)
print(drift_qvalue.iloc[-1])

"""## Simple Exponential Smoothing Method"""

def SimpleExponentialForecast(y_train,y_test,alpha,l0):
    y_train = y_train.tolist()
    y_test = y_test.tolist()
    train_pred = [l0]
    for i in range(1,len(y_train)):
        train_pred.append(alpha*y_train[i-1]+(1-alpha)*train_pred[i-1])
    train_error = np.array(y_train[2:])-np.array(train_pred[2:])
    test_pred = []
    for j in range(1,len(y_test)+1):
        test_pred.append(alpha*y_train[-1]+(1-alpha)*train_pred[-1])
    test_error = np.array(y_test[2:])-np.array(test_pred[2:])
    return train_pred,test_pred,train_error,test_error

"""### Tuning parameter for SES"""

alpha_lst=[0.2,0.5,0.8]
l0_lst=[0,1000,1500]
sem_tab_index=[]
sem_mse_arr=np.zeros((len(alpha_lst)*len(l0_lst),2))
i=0
for al in range(len(alpha_lst)):
  for l in range(len(l0_lst)):
    sem_train_pred,sem_test_pred,sem_train_err,sem_test_err = SimpleExponentialForecast(y_train,y_test,alpha_lst[int(al)],l0_lst[l])
    sem_mse_arr[i]=[np.mean(sem_train_err**2),np.mean(sem_test_err**2)]
    sem_tab_index.append('alpha ='+str(alpha_lst[al])+', l0 ='+str(l0_lst[l]))
    i+=1
ses_tun_tab=pd.DataFrame(sem_mse_arr,index=sem_tab_index)
ses_tun_tab
print(tabulate(ses_tun_tab,headers=["Parameters", "Prediction MSE","Forecast MSE"]))

"""**Alpha value of 0.8 and l0 value of 100 gives the lowest Prediction MSE as well as Forecase MSE among all parameters, we will graph it to see how it work**"""

alpha=0.8
l0=1000
sem_train_pred,sem_test_pred,sem_train_err,sem_test_err = SimpleExponentialForecast(y_train,y_test,alpha,l0)
plt.plot(y_train,color="blue",label="Training Dataset")
plt.plot(y_test,color="orange",label="Testing Dataset")
plt.plot(y_test.index,sem_test_pred,color="green",label="Simple Exponential Method in h-step Prediction")
plt.legend()
plt.title("Simple Exponential Method & Forecast")
plt.xlabel("time - t")
plt.ylabel("y value")
plt.show()

#Prediction and Forecast MSE with alpha=0.8, l0=1000
sem_pred_mse=np.mean(sem_train_err**2)
sem_forecast_mse=np.mean(sem_test_err**2)
print(tabulate([["Prediction",sem_pred_mse],["Forecast",sem_forecast_mse]],headers=["Simple Exponential Method MSE"]))

#Prediction and Forecast Variance
sem_pred_var=np.var(sem_train_err)
sem_forecast_var=np.var(sem_test_err)
print("The variance of simple exponential method prediction error is {}, \nand the variance of simple exponential method forecast error is {}"
      .format(sem_pred_var,sem_forecast_var))

#Q-value
sem_qvalue=sm.stats.acorr_ljungbox(sem_train_err,lags=n_lag,return_df=True)
print(sem_qvalue.iloc[-1])

"""# Time Series Modeling

## ARMA Model

**Let's start with the original y test. Since we have already performed a ACF/PACF Analysis, ADF and KPSS test as well as Rolling Mean Variane Plots, we've observed that the data is non-stationary**
"""

def cal_GPAC(ry2,k_max,j_max):
    gpac=np.zeros((j_max+1,k_max))
    ry=ry2[:int((len(ry2)-1)/2)+1][::-1]
    for j in range(j_max+1):
        for k in range(1,k_max+1):
            if k==1:
                gpac[j,k-1]=ry[j+k]/ry[j]
            else:
                num=np.zeros((k,k))
                den=np.zeros((k,k))
                for n in range(k):
                    for m in range(k):
                        if m==k-1:
                            num[n,m]=ry[abs(j+n+1)]
                            den[n,m]=ry[abs(j-k+n+1)]
                        else:
                            num[n,m]=ry[abs(j+n-m)]
                            den[n,m]=ry[abs(j+n-m)]
                gpac[j,k-1]=la.det(num)/la.det(den)
    gpac_table=pd.DataFrame(gpac,columns=np.arange(1,k_max+1,1))
    return gpac_table

"""### ACF and PACF"""

ACF_PACF_Plot(y_train,50)

"""### GPAC Table"""

ry=sm.tsa.stattools.acf(y_train, nlags=n_lag)
ry1 = ry[::-1]
ry2 = np.concatenate((ry1, ry[1:]))
gpac_table=cal_GPAC(ry2,7,7)
plt.figure(figsize=(10, 10))
sns.heatmap(gpac_table, annot=True, fmt=".3f")
plt.title("Generalized Partial Autocorrelation(GPAC) Table")
plt.tight_layout()
plt.show()

"""**The GPAC Table displays relavantly constant values at the first column, while a relavantly row of zeros occurs at row 0. Although this pattern is not strictly followed, we want to try fitting a model ARMA(1,0) to see how it performs.**

### Estimated Model with LM Algorithm
"""

def display_lm_result(model,na,nb):
  for i in range(1,na+1):
    print("The AR coefficient a{}".format(i),"is:",f"{model.params[i-1]:.3f}")
  for i in range(1,nb+1):
      print("The MA coefficient b{}".format(i),"is:",f"{model.params[i+na-1]:.3f}")
  print("\nThe standard error of the estimated coefficients are:\n",model.bse)
  print("\nThe Confidence intervals are\n",model.conf_int().values)

na=1
nb=0
arma_model=sm.tsa.arima.ARIMA(y_train,order=(na,0,nb),trend='n').fit()
print(arma_model.summary())

display_lm_result(arma_model,na,nb)

"""### Model Evaluation"""

arma_resid=y_train-arma_model.predict()
ry=list(sm.tsa.stattools.acf(arma_resid, nlags=n_lag))
ryy=ry[::-1][:-1]+ry
m=1.96/np.sqrt(n_lag)
lag=np.arange(-n_lag,n_lag+1,1)
plt.stem(lag,ryy)
plt.axhspan(-m,m,alpha=0.2)
plt.grid()
plt.xlabel("lags")
plt.ylabel("magnitude")
plt.title("Plot of residual ACF")
plt.show()

arma_hat=arma_model.predict()
e=y_train-arma_hat
re=sm.tsa.stattools.acf(e, nlags=n_lag)
Q=len(y_test)*np.sum(np.square(re[n_lag:]))
DOF=n_lag-na-nb
alfa=0.01
chi_critical=chi2.ppf(1-alfa,DOF)

if Q<chi_critical:
    print("The residual is white")
else:
    print("The residual is NOT white")

Q

"""## ARIMA Model

### ACF and PACF
"""

ACF_PACF_Plot(y_diff1,70)

"""AR(1)24

### GPAC Table
"""

ry=sm.tsa.stattools.acf(y_diff1, nlags=n_lag)
ry1 = ry[::-1]
ry2 = np.concatenate((ry1, ry[1:]))
gpac_table=cal_GPAC(ry2,30,7)
plt.figure(figsize=(16, 16))
sns.heatmap(gpac_table, annot=True, fmt=".3f")
plt.title("Generalized Partial Autocorrelation(GPAC) Table")
plt.tight_layout()
plt.show()

"""**The GPAC Table indicate that na of 24 might be a potential order for AR with nb of 0.. Although this pattern is not strictly followed, we want to try fitting a model ARIMA(24,1,0) to see how it performs.**"""

na=24
nb=0
arima_model=sm.tsa.arima.ARIMA(y_train,order=(na,1,nb),trend='n').fit()
print(arima_model.summary())

display_lm_result(arima_model,na,nb)

arima_resid=y_train-arima_model.predict()
ry=list(sm.tsa.stattools.acf(arima_resid, nlags=n_lag))
ryy=ry[::-1][:-1]+ry
m=1.96/np.sqrt(n_lag)
lag=np.arange(-n_lag,n_lag+1,1)
plt.stem(lag,ryy)
plt.axhspan(-m,m,alpha=0.2)
plt.grid()
plt.xlabel("lags")
plt.ylabel("magnitude")
plt.title("Plot of residual ACF")
plt.show()

arima_hat=arima_model.predict()
e=y_train-arima_hat
re=sm.tsa.stattools.acf(e, nlags=n_lag)
Q=len(y_test)*np.sum(np.square(re[n_lag:]))
DOF=n_lag-na-nb
alfa=0.01
chi_critical=chi2.ppf(1-alfa,DOF)

if Q<chi_critical:
    print("The residual is white")
else:
    print("The residual is NOT white")

arima_qvalue=Q
print(Q)

plt.figure()
plt.plot(y,label='raw dataset')
plt.plot(arima_hat,label='1-step ahead prediction')
plt.title("train vs. 1-step prediction")
plt.xlabel("time")
plt.ylabel("y value")
plt.legend()
plt.show()

arima_hat_h=arima_model.forecast(steps=len(y_test))
e_h=y_test[:-1]-arima_hat_h
print("Variance of test versus prediction is :",np.var(y_test)/np.var(arima_hat_h))
plt.figure()
plt.plot(y_test,label='test set')
plt.plot(arima_hat_h,label='h-step ahead prediction')
plt.title("test vs. 1-step prediction")
plt.xlabel("time")
plt.ylabel("y value")
plt.legend()
plt.show()

"""## SARIMA Model

### ACF and PACF
"""

s_period=24
y_sdiff=y_train.diff(periods=s_period)[s_period:]
ACF_PACF_Plot(y_sdiff,50)

"""**AR(1) X MA(1)24**

Non-seasonal part:(1,0,0)

Seasonal part:(0,1,1)24

ARMIMA(2,0,0)*ARIMA(0,1,1)24

### GPAC Table
"""

ry=sm.tsa.stattools.acf(y_sdiff, nlags=n_lag)
ry1 = ry[::-1]
ry2 = np.concatenate((ry1, ry[1:]))
gpac_table=cal_GPAC(ry2,5,30)
plt.figure(figsize=(16, 16))
sns.heatmap(gpac_table, annot=True, fmt=".3f")
plt.title("Generalized Partial Autocorrelation(GPAC) Table")
plt.tight_layout()
plt.show()

"""### Try sqrt transform on y"""

sarima_model=sm.tsa.SARIMAX(y_train,order=(2,0,0),seasonal_order=(0,1,1,24)).fit()
print(sarima_model.summary())

"""### Diagnostic Teset"""

na=2
nb=1
display_lm_result(sarima_model,na,nb)

print(f"The roots of denominator are {np.roots([1,1.064,-0.2022])}")
print(f"The roots of numerator are {np.roots([1,-0.940])}")

"""### 1-step ahead prediction """

sarima_resid=y_train-sarima_model.predict()
ry=list(sm.tsa.stattools.acf(sarima_resid, nlags=n_lag))
ryy=ry[::-1][:-1]+ry
m=1.96/np.sqrt(n_lag)
lag=np.arange(-n_lag,n_lag+1,1)
plt.stem(lag,ryy)
plt.axhspan(-m,m,alpha=0.2)
plt.grid()
plt.xlabel("lags")
plt.ylabel("magnitude")
plt.title("Plot of residual ACF")
plt.show()

sarima_hat=sarima_model.predict(start=1,end=len(y_train)-1)
e=y_train[s_period:]-sarima_hat[-s_period]
re=list(sm.tsa.stattools.acf(e[1:-1], nlags=n_lag))
Q=len(y_sdiff)*np.sum(np.square(re[n_lag:]))
DOF=n_lag-na-nb
alfa=0.01
chi_critical=chi2.ppf(1-alfa,DOF)

if Q<chi_critical:
    print("The residual is white")
else:
    print("The residual is NOT white")

e

plt.figure()
plt.plot(y_train,label='raw dataset')
plt.plot(sarima_hat,label='1-step ahead prediction')
plt.title("train vs. 1-step prediction")
plt.xlabel("time")
plt.ylabel("y value")
plt.legend()
plt.show()

plt.figure()
plt.plot(y_train[:100],label='test set')
plt.plot(sarima_hat[:100],label='1-step ahead prediction')
plt.title("train vs. 1-step prediction")
plt.xlabel("time")
plt.ylabel("y value")
plt.legend()
plt.show()

"""## h-step prediction"""

sarima_hat_h=sarima_model.forecast(steps=len(y_test))
e_h=y_test[:-1]-sarima_hat_h
print("Variance of test versus prediction is :",np.var(y_test)/np.var(sarima_hat_h))
print("h-step prediction MSE is:",np.mean((y_test-e_h)**2))

""" ar.L1         0.006381
ar.L2         0.008770
ma.S.L24      0.002339
sigma2      270.356754
"""

sarima_hat_h=sarima_model.forecast(steps=len(y_test))

e_h=y_test[:-1]-sarima_hat_h
print("Variance of test versus prediction is :",np.var(y_test)/np.var(sarima_hat_h))
plt.figure()
plt.plot(y_test,label='test set')
plt.plot(sarima_hat_h,label='h-step ahead prediction')
plt.title("test vs. h-step prediction")
plt.xlabel("time")
plt.ylabel("y value")
plt.legend()
plt.show()

sarima_hat_h=sarima_model.forecast(steps=len(y_test))**2

e_h=y_test[:-1]-sarima_hat_h
print("Variance of test versus prediction is :",np.var(y_test)/np.var(sarima_hat_h))
plt.figure()
plt.plot(y_test[:100],label='test set')
plt.plot(sarima_hat_h[:100],label='h-step ahead prediction')
plt.title("test vs. 1-step prediction")
plt.xlabel("time")
plt.ylabel("y value")
plt.legend()
plt.show()

"""## SARIMA Model with Order and Seasonal Differencing

### ACF and PACF
"""

y_sdiff1=y_diff1.diff(periods=s_period)[s_period:]
ACF_PACF_Plot(y_sdiff1,100)

"""**AR(0) X MA(2)24**"""

ry=sm.tsa.stattools.acf(y_sdiff1, nlags=n_lag)
ry1 = ry[::-1]
ry2 = np.concatenate((ry1, ry[1:]))
gpac_table=cal_GPAC(ry2,6,30)
plt.figure(figsize=(10, 10))
sns.heatmap(gpac_table, annot=True, fmt=".3f")
plt.title("Generalized Partial Autocorrelation(GPAC) Table")
plt.tight_layout()
plt.show()

#sarima_model1=sm.tsa.SARIMAX(y_train,order=(0,1,0),seasonal_order=(0,1,2,24)).fit()
#print(sarima_model1.summary())

